{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113affa5-6169-4c77-8f4f-c2afbba7c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.io import loadmat, savemat\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8de8ef-5670-4188-b8b4-5f15c0bda431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type 'LP' for LinkNet + PSPNet segmentation \n",
      "Type 'PP' for PSPNet + PSPNet segmentation \n",
      "Type 'V' for vessels segmentation\n",
      " V\n"
     ]
    }
   ],
   "source": [
    "model_preference = input(\"Type 'LP' for LinkNet + PSPNet segmentation \\nType 'PP' for PSPNet + PSPNet segmentation \\nType 'V' for vessels segmentation\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18f8800-66a6-4c9a-8185-6d2906cf999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type 'all' to perform segmentation for all images \n",
      "Type name of image for single image segmentation\n",
      " vesselsA\n"
     ]
    }
   ],
   "source": [
    "type_of_segmentation = input(\"Type 'all' to perform segmentation for all images \\nType name of image for single image segmentation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77a065-f097-46a0-83d2-8727eaf89a73",
   "metadata": {},
   "source": [
    "# Segmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ce1cb3-ea22-4e56-97e5-f369e0263d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# torch.cuda.empty_cache()\n",
    "transform = transforms.ToTensor()\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# #torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "\n",
    "# print(torch.cuda.get_device_properties(device).multi_processor_count)\n",
    "# torch.cuda.memory_allocated(device=torch.device(\"cuda\"))\n",
    "# torch.cuda.memory_summary(device=torch.device(\"cuda\"), abbreviated=False)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, padding=0, stride=1, dilation=1, bias=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        padding = (kernel_size + (kernel_size - 1) * (dilation - 1)) // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convInst = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-2] == 1 and x.shape[-1]==1:\n",
    "             out = self.convInst(x)\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "        return out\n",
    "    \n",
    "    def upsample(input, size=None, scale_factor=None, align_corners=False):\n",
    "        out = F.interpolate(input, size=size, scale_factor=scale_factor, mode='bilinear', align_corners=align_corners)\n",
    "        return out\n",
    "        \n",
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        self.pooling_size = [1, 2, 3, 6]\n",
    "        self.channels = in_channels // 4\n",
    "        \n",
    "        self.pool1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(self.pooling_size[0]),\n",
    "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.pool2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(self.pooling_size[1]),\n",
    "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.pool3 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(self.pooling_size[2]),\n",
    "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.pool4 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(self.pooling_size[3]),\n",
    "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.pool1(x)\n",
    "        out1 = upsample(out1, size=x.size()[-2:])\n",
    "\n",
    "        out2 = self.pool2(x)\n",
    "        out2 = upsample(out2, size=x.size()[-2:])\n",
    "\n",
    "        out3 = self.pool3(x)\n",
    "        out3 = upsample(out3, size=x.size()[-2:])\n",
    "\n",
    "        out4 = self.pool4(x)\n",
    "        out4 = upsample(out4, size=x.size()[-2:])\n",
    "\n",
    "        out = torch.cat([x, out1, out2, out3, out4], dim=1)\n",
    "        return out\n",
    "        \n",
    "def upsample(input, size=None, scale_factor=None, align_corners=False):\n",
    "    out = F.interpolate(input, size=size, scale_factor=scale_factor, mode='bilinear', align_corners=align_corners)\n",
    "    return out\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=64, n_out_classes=3):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.out_channels = 512  #2048\n",
    "\n",
    "        self.backbone = resnet.resnet34(pretrained=True)\n",
    "        self.stem = nn.Sequential(\n",
    "            *list(self.backbone.children())[:4],\n",
    "        )\n",
    "        self.block1 = self.backbone.layer1\n",
    "        self.block2 = self.backbone.layer2\n",
    "        self.block3 = self.backbone.layer3\n",
    "        self.block4 = self.backbone.layer4\n",
    "        #self.low_level_features_conv = ConvBlock(64, 64, kernel_size=3)\n",
    "\n",
    "        self.depth = self.out_channels // 4\n",
    "        self.pyramid_pooling = PyramidPooling(self.out_channels)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            ConvBlock(self.out_channels * 2, self.depth, kernel_size=3),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(self.depth, n_out_classes, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.aux = nn.Sequential(\n",
    "            ConvBlock(self.out_channels // 2, self.depth // 2, kernel_size=3),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(self.depth // 2, n_classes, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.sftmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x, label=None):\n",
    "        out = self.stem(x)\n",
    "        out1 = self.block1(out)\n",
    "        out2 = self.block2(out1)\n",
    "        out3 = self.block3(out2)\n",
    "        out4 = self.block4(out3)\n",
    "        \n",
    "        out = self.pyramid_pooling(out4)\n",
    "        out = self.decoder(out)\n",
    "        out = upsample(out, size=x.size()[-2:])\n",
    "        out = upsample(out, size=x.shape[-2:], align_corners=True)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, groups=1, bias=False, dropout_rate = 0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size, 1, padding, groups=groups, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        self.downsample = None\n",
    "        if stride > 1:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                            nn.BatchNorm2d(out_planes),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, groups=1, bias=False, dropout_rate = 0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.block1 = BasicBlock(in_planes, out_planes, kernel_size, stride, padding, groups, bias, dropout_rate)\n",
    "        self.block2 = BasicBlock(out_planes, out_planes, kernel_size, 1, padding, groups, bias, dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=False):\n",
    "        # TODO bias=True\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_planes, in_planes//4, 1, 1, 0, bias=bias),\n",
    "                                nn.BatchNorm2d(in_planes//4),\n",
    "                                nn.ReLU(inplace=True),)\n",
    "        self.tp_conv = nn.Sequential(nn.ConvTranspose2d(in_planes//4, in_planes//4, kernel_size, stride, padding, output_padding, bias=bias),\n",
    "                                nn.BatchNorm2d(in_planes//4),\n",
    "                                nn.ReLU(inplace=True),)\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_planes//4, out_planes, 1, 1, 0, bias=bias),\n",
    "                                nn.BatchNorm2d(out_planes),\n",
    "                                nn.ReLU(inplace=True),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.tp_conv(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinkNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate model architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=1, num_channels = 1):\n",
    "        \"\"\"\n",
    "        Model initialization\n",
    "        :param x_n: number of input neurons\n",
    "        :type x_n: int\n",
    "        \"\"\"\n",
    "        super(LinkNetBase, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, 7, 2, 3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
    "        \n",
    "        self.encoder1 = Encoder(64, 64, 3, 1, 1)\n",
    "        self.encoder2 = Encoder(64, 128, 3, 2, 1)\n",
    "        self.encoder3 = Encoder(128, 256, 3, 2, 1)\n",
    "        #self.encoder3 = Encoder(128, 256, 3, 2, 1, 1, False, 0.1)\n",
    "        self.encoder4 = Encoder(256, 512, 3, 2, 1)\n",
    "        self.encoder5 = Encoder(512, 1024, 3, 2, 1)\n",
    "        #self.encoder5 = Encoder(512, 1024, 3, 2, 1, 1, False, 0.5)\n",
    "        \n",
    "        self.decoder1 = Decoder(64, 64, 3, 1, 1, 0)\n",
    "        self.decoder2 = Decoder(128, 64, 3, 2, 1, 1)\n",
    "        self.decoder3 = Decoder(256, 128, 3, 2, 1, 1)\n",
    "        self.decoder4 = Decoder(512, 256, 3, 2, 1, 1)\n",
    "        self.decoder5 = Decoder(1024, 512, 3, 2, 1, 1)\n",
    "\n",
    "        # Classifier\n",
    "        self.tp_conv1 = nn.Sequential(nn.ConvTranspose2d(64, 32, 3, 2, 1, 1),\n",
    "                                      nn.BatchNorm2d(32),\n",
    "                                      nn.ReLU(inplace=True),)\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(inplace=True),)\n",
    "        self.tp_conv2 = nn.ConvTranspose2d(32, n_classes, 2, 2, 0)\n",
    "        \n",
    "        self.conv3 =  nn.Sequential(nn.Conv2d(16, 16, 2, 2, 0),\n",
    "                                nn.BatchNorm2d(16),\n",
    "                                nn.ReLU(inplace=True),)\n",
    "        self.tp_conv3 = nn.ConvTranspose2d(16, n_classes, 2, 2, 0)\n",
    "\n",
    "        \n",
    "        self.lsm = nn.LogSoftmax(dim=1) #Sigmoid/SoftMax\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.sftmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    # def conv_to_match_channels(self, input_tensor, target_tensor):\n",
    "    #     # Apply a convolution to match the number of channels in input_tensor with target_tensor\n",
    "    #     num_channels_input = input_tensor.shape[1]\n",
    "    #     num_channels_target = target_tensor.shape[1]\n",
    "    \n",
    "    #     if num_channels_input < num_channels_target:\n",
    "    #         # Apply a convolution to increase the number of channels\n",
    "    #         conv_layer = nn.Conv2d(num_channels_input, num_channels_target, kernel_size=1, padding=0)\n",
    "    #     elif num_channels_input > num_channels_target:\n",
    "    #         # Apply a convolution to decrease the number of channels\n",
    "    #         conv_layer = nn.Conv2d(num_channels_input, num_channels_target, kernel_size=1, padding=0)\n",
    "    #     else:\n",
    "    #         # No need to change channels\n",
    "    #         return input_tensor\n",
    "    \n",
    "    #     return F.relu(conv_layer(input_tensor))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Encoder blocks\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        \n",
    "        # Decoder blocks\n",
    "        d4 = e3 + self.decoder4(e4)\n",
    "        d3 = e2 + self.decoder3(d4)\n",
    "        d2 = e1 + self.decoder2(d3)\n",
    "        d1 = x + self.decoder1(d2)\n",
    "\n",
    "        # Classifier\n",
    "        y = self.tp_conv1(d1)\n",
    "        y = self.conv2(y)\n",
    "        y = self.tp_conv2(y)\n",
    "        y = self.sigm(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "folder = rf'{os.getcwd()}' #r'../RetinalSegmentation'\n",
    "\n",
    "linknet_model_disk = torch.load(r''+folder+'/Models/LinkNet_Disk/LinkNet_Disk.pt', map_location=torch.device('cpu'))\n",
    "pspnet_model_cup = torch.load(r''+folder+'/Models/PSPNet_Cup/PSPNet_resnet34_pretrained_combined_255.pt', map_location=torch.device('cpu'))\n",
    "pspnet_model_disk = torch.load(r''+folder+'/Models/PSPNet_Disk/PSPNet_resnet34_pretrained.pt', map_location=torch.device('cpu'))\n",
    "vessels_model = torch.load(r''+folder+'/Models/LinkNet_Veins/LinkNet_Veins.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "test_folder = folder + '/Images'\n",
    "test_results_folder = folder + '/Segmented_Masks'\n",
    "test_results_overlayed_folder = folder + '/Overlayed_Segmented_Masks'\n",
    "\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "images_test = []\n",
    "for file in test_files:\n",
    "    images_test.append(test_folder + \"/\" + file)\n",
    "\n",
    "threshold = 0.5\n",
    "if model_preference == 'LP':\n",
    "    if type_of_segmentation == 'all':\n",
    "        linknet_model_disk.eval()\n",
    "        pspnet_model_cup.eval()\n",
    "        with torch.no_grad():\n",
    "            for image in images_test:\n",
    "                filename = os.path.basename(image)\n",
    "                image = Image.open(image)\n",
    "                image_disk = image.resize((512, 512), resample=Image.NEAREST)\n",
    "                image_disk = image_disk.convert('L')\n",
    "                image_disk = transform(image_disk)\n",
    "                image_disk = image_disk.unsqueeze(0)\n",
    "\n",
    "                output_disk = linknet_model_disk(image_disk)\n",
    "                \n",
    "                output_disk = output_disk.detach().numpy()[0]\n",
    "                output_disk = (output_disk > threshold)\n",
    "                output_disk = output_disk.astype(np.uint8)\n",
    "                output_disk = np.transpose(output_disk, (1, 2, 0))\n",
    "                \n",
    "                output_disk = Image.fromarray(output_disk.squeeze())\n",
    "                linknet_disk = output_disk.resize((256, 256), resample=Image.NEAREST)\n",
    "                linknet_disk = np.array(linknet_disk)\n",
    "        \n",
    "                image_cup = image.resize((256, 256), resample=Image.NEAREST)\n",
    "                image_cup = transform(image_cup)\n",
    "                image_cup = image_cup.unsqueeze(0)\n",
    "                \n",
    "                #image_cup = image_cup.to(device) \n",
    "                output_cup = pspnet_model_cup(image_cup)\n",
    "                \n",
    "                output_cup = output_cup.detach().numpy()[0]\n",
    "                output_cup = (output_cup > threshold)\n",
    "                \n",
    "                output_cup = np.transpose(output_cup, (1, 2, 0))\n",
    "                pspnet_cup = output_cup[:,:,2]\n",
    "                pspnet_cup = pspnet_cup.astype(np.uint8)\n",
    "        \n",
    "                merged_mask = np.zeros_like(linknet_disk)\n",
    "                merged_mask[linknet_disk == 1] = 128\n",
    "                merged_mask[pspnet_cup == 1] = 255\n",
    "\n",
    "                overlay_image_predict = np.copy(image.resize((256, 256), resample=Image.NEAREST))\n",
    "                overlay_image_predict[linknet_disk == 1] = 128\n",
    "                overlay_image_predict[pspnet_cup == 1] = 255\n",
    "\n",
    "                cv2.imwrite(test_results_folder+\"/\"+filename, merged_mask)\n",
    "                cv2.imwrite(test_results_overlayed_folder+\"/\"+filename, overlay_image_predict)\n",
    "\n",
    "    else:\n",
    "        orig_image = Image.open(test_folder + \"/\" + type_of_segmentation + \".jpg\")\n",
    "        \n",
    "        linknet_model_disk.eval()\n",
    "        image = orig_image.resize((512, 512), resample=Image.NEAREST)\n",
    "        image = image.convert('L')\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        output = linknet_model_disk(image)\n",
    "        \n",
    "        output = output.detach().numpy()[0]\n",
    "        output = (output > threshold)\n",
    "        output = output.astype(np.uint8)\n",
    "        output = np.transpose(output, (1, 2, 0))\n",
    "        \n",
    "        output = Image.fromarray(output.squeeze())\n",
    "        linknet_disk = output.resize((256, 256), resample=Image.NEAREST)\n",
    "        linknet_disk = np.array(linknet_disk)\n",
    "\n",
    "        pspnet_model_cup.eval()\n",
    "        image = orig_image.resize((256, 256), resample=Image.NEAREST)\n",
    "        #image = image.convert('L')\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        #image = image.to(device) \n",
    "        output = pspnet_model_cup(image)\n",
    "        \n",
    "        output = output.detach().numpy()[0]\n",
    "        output = (output > threshold)\n",
    "        output = np.transpose(output, (1, 2, 0))\n",
    "        pspnet_cup = output[:,:,2]\n",
    "        pspnet_cup = pspnet_cup.astype(np.uint8)\n",
    "\n",
    "        merged_mask = np.zeros_like(linknet_disk)\n",
    "        merged_mask[linknet_disk == 1] = 128\n",
    "        merged_mask[pspnet_cup == 1] = 255\n",
    "\n",
    "        overlay_image_predict = np.copy(orig_image.resize((256, 256), resample=Image.NEAREST))\n",
    "        overlay_image_predict[linknet_disk == 1] = 128\n",
    "        overlay_image_predict[pspnet_cup == 1] = 255\n",
    "        plt.imshow(overlay_image_predict)\n",
    "        cv2.imwrite(test_results_folder+\"/\"+type_of_segmentation+\".jpg\", merged_mask)\n",
    "        cv2.imwrite(test_results_overlayed_folder+\"/\"+type_of_segmentation+\".jpg\", overlay_image_predict)\n",
    "\n",
    "if model_preference == 'PP':\n",
    "    if type_of_segmentation == 'all':\n",
    "        pspnet_model_disk.eval()\n",
    "        pspnet_model_cup.eval()\n",
    "        with torch.no_grad():\n",
    "            for image in images_test:\n",
    "                filename = os.path.basename(image)\n",
    "                image = Image.open(image)\n",
    "                \n",
    "                image_disk = image.resize((256, 256), resample=Image.NEAREST)\n",
    "                image_disk = transform(image_disk)\n",
    "                image_disk = image_disk.unsqueeze(0)\n",
    "                \n",
    "                #image_disk = image_disk.to(device) \n",
    "                output_disk = pspnet_model_disk(image_disk)\n",
    "                \n",
    "                output_disk = output_disk.detach().numpy()[0]\n",
    "                output_disk = (output_disk > threshold)\n",
    "                output_disk = np.transpose(output_disk, (1, 2, 0))\n",
    "                pspnet_disk = output_disk.squeeze()\n",
    "                pspnet_disk = pspnet_disk.astype(np.uint8)\n",
    "        \n",
    "                image_cup = image.resize((256, 256), resample=Image.NEAREST)\n",
    "                image_cup = transform(image_cup)\n",
    "                image_cup = image_cup.unsqueeze(0)\n",
    "                \n",
    "                output_cup = pspnet_model_cup(image_cup)\n",
    "                \n",
    "                output_cup = output_cup.detach().numpy()[0]\n",
    "                output_cup = (output_cup > threshold)\n",
    "                \n",
    "                output_cup = np.transpose(output_cup, (1, 2, 0))\n",
    "                pspnet_cup = output_cup[:,:,2]\n",
    "                pspnet_cup = pspnet_cup.astype(np.uint8)\n",
    "        \n",
    "                merged_mask = np.zeros_like(pspnet_disk)\n",
    "                merged_mask[pspnet_disk == 1] = 128\n",
    "                merged_mask[pspnet_cup == 1] = 255\n",
    "\n",
    "                overlay_image_predict = np.copy(image.resize((256, 256), resample=Image.NEAREST))\n",
    "                overlay_image_predict[pspnet_disk == 1] = 128\n",
    "                overlay_image_predict[pspnet_cup == 1] = 255\n",
    "\n",
    "                cv2.imwrite(test_results_folder+\"/\"+filename, merged_mask)\n",
    "                cv2.imwrite(test_results_overlayed_folder+\"/\"+filename, overlay_image_predict)\n",
    "\n",
    "    else:\n",
    "        orig_image = Image.open(test_folder + \"/\" + type_of_segmentation + \".jpg\")\n",
    "        \n",
    "        pspnet_model_disk.eval()\n",
    "        image = orig_image.resize((256, 256), resample=Image.NEAREST)\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        output = pspnet_model_disk(image)\n",
    "        \n",
    "        output = output.detach().numpy()[0]\n",
    "        output = (output > threshold)\n",
    "        \n",
    "        output = np.transpose(output, (1, 2, 0))\n",
    "        pspnet_disk = output.squeeze()\n",
    "        pspnet_disk = pspnet_disk.astype(np.uint8)\n",
    "\n",
    "        pspnet_model_cup.eval()\n",
    "        image = orig_image.resize((256, 256), resample=Image.NEAREST)\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        output = pspnet_model_cup(image)\n",
    "        \n",
    "        output = output.detach().numpy()[0]\n",
    "        output = (output > threshold)\n",
    "        \n",
    "        output = np.transpose(output, (1, 2, 0))\n",
    "        pspnet_cup = output[:,:,2]\n",
    "        pspnet_cup = pspnet_cup.astype(np.uint8)\n",
    "\n",
    "        merged_mask = np.zeros_like(pspnet_disk)\n",
    "        merged_mask[pspnet_disk == 1] = 128\n",
    "        merged_mask[pspnet_cup == 1] = 255\n",
    "\n",
    "        overlay_image_predict = np.copy(orig_image.resize((256, 256), resample=Image.NEAREST))\n",
    "        overlay_image_predict[pspnet_disk == 1] = 128\n",
    "        overlay_image_predict[pspnet_cup == 1] = 255\n",
    "\n",
    "        cv2.imwrite(test_results_folder+\"/\"+type_of_segmentation+\".jpg\", merged_mask)\n",
    "        cv2.imwrite(test_results_overlayed_folder+\"/\"+type_of_segmentation+\".jpg\", overlay_image_predict)\n",
    "        \n",
    "if model_preference == 'V':\n",
    "    if type_of_segmentation == 'all':\n",
    "        vessels_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for image in images_test:\n",
    "                filename = os.path.basename(image)\n",
    "                image = Image.open(image)\n",
    "                vessels_image = image.resize((512, 512), resample=Image.NEAREST)\n",
    "                vessels_image = vessels_image.convert('L')\n",
    "                vessels_image = transform(vessels_image)\n",
    "                #image = image/255\n",
    "                vessels_image = vessels_image.unsqueeze(0)\n",
    "                vessels_output = vessels_model(vessels_image)\n",
    "        \n",
    "                vessels_output = vessels_output.detach().numpy()[0]\n",
    "                vessels_output = (vessels_output > threshold)\n",
    "                \n",
    "                vessels_output = np.transpose(vessels_output, (1, 2, 0))\n",
    "                vessels_output = np.array(vessels_output)\n",
    "                vessels_output = np.repeat((vessels_output == 1).astype(np.uint8), 3, axis=2)\n",
    "                vessels_output[vessels_output==1] = 255\n",
    "                \n",
    "                overlay_image_predict = np.copy(image.resize((512, 512), resample=Image.NEAREST))\n",
    "                overlay_image_predict[vessels_output == 255] = 255\n",
    "\n",
    "                cv2.imwrite(test_results_folder+\"/\"+filename, vessels_output)\n",
    "                cv2.imwrite(test_results_overlayed_folder+\"/\"+filename, overlay_image_predict)\n",
    "\n",
    "    else:\n",
    "        orig_image = Image.open(test_folder + \"/\" + type_of_segmentation + \".jpg\")\n",
    "        vessels_model.eval()\n",
    "        image = orig_image.resize((512, 512), resample=Image.NEAREST)\n",
    "        image = image.convert('L')\n",
    "        image = transform(image)\n",
    "        #image = image/255\n",
    "        image = image.unsqueeze(0)\n",
    "        vessels_output = vessels_model(image)\n",
    "\n",
    "        vessels_output = vessels_output.detach().numpy()[0]\n",
    "        vessels_output = (vessels_output > threshold)\n",
    "        \n",
    "        vessels_output = np.transpose(vessels_output, (1, 2, 0))\n",
    "        vessels_output = np.array(vessels_output)\n",
    "        vessels_output = np.repeat((vessels_output == 1).astype(np.uint8), 3, axis=2)\n",
    "        vessels_output[vessels_output==1] = 255\n",
    "\n",
    "        overlay_image_predict = np.copy(orig_image.resize((512, 512), resample=Image.NEAREST))\n",
    "        overlay_image_predict[vessels_output == 255] = 255\n",
    "\n",
    "        cv2.imwrite(test_results_folder+\"/\"+type_of_segmentation+\".jpg\", vessels_output)\n",
    "        cv2.imwrite(test_results_overlayed_folder+\"/\"+type_of_segmentation+\".jpg\", overlay_image_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
